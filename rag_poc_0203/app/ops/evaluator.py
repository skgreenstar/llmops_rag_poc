from typing import List, Dict, Any, Optional
import asyncio
from langfuse import Langfuse
from app.core.config import get_settings

# Ragas & LangChain imports
from app.ops.monitor import observable, langfuse_context
from ragas import evaluate
from ragas.metrics import (
    faithfulness, 
    answer_relevancy, 
    answer_correctness,
    context_precision,
    context_recall,
    context_entity_recall,
    answer_similarity
)
from ragas.metrics._aspect_critic import conciseness, coherence, harmfulness, maliciousness
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama, OllamaEmbeddings
from datasets import Dataset
import traceback

# Reference: https://langfuse.com/guides/cookbook/evaluation_of_rag_with_ragas

settings = get_settings()

class EvaluationResult:
    def __init__(self, score: float, reasoning: str, metric_name: str):
        self.score = score
        self.reasoning = reasoning
        self.metric_name = metric_name

class Evaluator:
    def __init__(self):
        self.langfuse = Langfuse(
            public_key=settings.LANGFUSE_PUBLIC_KEY,
            secret_key=settings.LANGFUSE_SECRET_KEY,
            host=settings.LANGFUSE_HOST
        )
        
        # Determine which LLM to use for evaluation
        # Fallback to local model if OpenAI key is invalid or placeholder
        api_key = settings.OPENAI_API_KEY
        self.is_placeholder = api_key is None or api_key == "" or api_key.startswith("sk-...")
        
        if not self.is_placeholder:
            print("ðŸš€ Using OpenAI for Ragas evaluation")
            llm = ChatOpenAI(
                model=settings.DEFAULT_MODEL_NAME,
                openai_api_key=api_key,
                temperature=0
            )
            embeddings = OpenAIEmbeddings(
                openai_api_key=api_key
            )
        else:
            print(f"ðŸ  Using Local Model ({settings.LOCAL_MODEL_NAME}) and Embeddings ({settings.EMBEDDING_MODEL}) for Ragas evaluation")
            llm = ChatOllama(
                model=settings.LOCAL_MODEL_NAME,
                base_url=settings.LOCAL_MODEL_URL,
                temperature=0
            )
            embeddings = OllamaEmbeddings(
                model=settings.EMBEDDING_MODEL,
                base_url=settings.EMBEDDING_BINDING_HOST
            )
            
        # Initialize LLM & Embeddings for Ragas with wrappers as recommended
        self.eval_llm = LangchainLLMWrapper(llm)
        self.eval_embeddings = LangchainEmbeddingsWrapper(embeddings)
        
        # Define the set of metrics to run (Full Suite)
        self.metrics = [
            faithfulness, 
            answer_relevancy, 
            answer_correctness,
            context_precision,
            context_recall,
            context_entity_recall,
            answer_similarity,
            conciseness,
            coherence,
            harmfulness,
            maliciousness
        ]

    def submit_score(self, trace_id: str, result: EvaluationResult):
        """
        Submits a score to Langfuse attached to a specific trace.
        """
        try:
            # Avoid submitting NaN values to Langfuse as it causes Bad Request errors
            import math
            if math.isnan(result.score):
                print(f"âš ï¸ Skipping NaN score: {result.metric_name}")
                return

            self.langfuse.score(
                trace_id=trace_id,
                name=result.metric_name,
                value=result.score,
                comment=result.reasoning
            )
            self.langfuse.flush()
            print(f"âœ… Score submitted: {result.metric_name} = {result.score:.2f}")
        except Exception as e:
            print(f"âŒ Failed to submit score: {e}")

    @observable(name="ragas_eval", as_type="span")
    async def run_ragas_eval(self, query: str, context: str, answer: str, reference: Optional[str] = None) -> List[EvaluationResult]:
        """
        Runs multiple Ragas metrics in a single batch.
        """
        # If reference is not provided, use query as a neutral reference 
        # (Note: This might lower correctness scores if they expect a specific ground truth)
        if reference is None:
            reference = query
            
        data = {
            "question": [query],
            "contexts": [[context]],
            "answer": [answer],
            "reference": [reference]
        }
        dataset = Dataset.from_dict(data)
        
        # Run evaluation
        try:
            # Update trace metadata with judge info
            if langfuse_context:
                langfuse_context.update_current_trace(
                    metadata={
                        "judge_llm": settings.DEFAULT_MODEL_NAME if not self.is_placeholder else settings.LOCAL_MODEL_NAME,
                        "judge_provider": "openai" if not self.is_placeholder else "ollama",
                        "metrics_count": len(self.metrics)
                    }
                )

            # We use wait_for to avoid hanging if there are network issues
            result = evaluate(
                dataset=dataset,
                metrics=self.metrics,
                llm=self.eval_llm,
                embeddings=self.eval_embeddings
            )
            
            # Ragas 0.2.x returns an EvaluationResult object. 
            # We convert the first row to a dict to get scalar scores.
            df = result.to_pandas()
            if df.empty:
                print("âš ï¸ Ragas evaluation returned an empty result.")
                return []
            
            scores = df.iloc[0].to_dict()
            
            output = []
            
            # Mapping of internal keys to Korean display names and detailed reasoning
            metric_info = {
                "faithfulness": {
                    "display_name": "ì¶©ì‹¤ë„ (Faithfulness)",
                    "reasoning": "ë‹µë³€ì´ ì£¼ì–´ì§„ ë¬¸ë§¥ì— ì–¼ë§ˆë‚˜ ì¶©ì‹¤í•˜ê²Œ ê·¼ê±°í•˜ê³  ìžˆëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)."
                },
                "answer_relevancy": {
                    "display_name": "ë‹µë³€ ê´€ë ¨ì„± (Answer Relevancy)",
                    "reasoning": "ë‹µë³€ì´ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì–¼ë§ˆë‚˜ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ í•´ê²°ì±…ì„ ì œì‹œí•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "answer_correctness": {
                    "display_name": "ë‹µë³€ ì •í™•ë„ (Answer Correctness)",
                    "reasoning": "ìƒì„±ëœ ë‹µë³€ì´ ê¸°ì¤€ ì •ë‹µ(Ground Truth)ê³¼ ë¹„êµí–ˆì„ ë•Œ ì‚¬ì‹¤ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "context_precision": {
                    "display_name": "ë¬¸ë§¥ ì •ë°€ë„ (Context Precision)",
                    "reasoning": "ê²€ìƒ‰ëœ ë¬¸ë§¥ ì •ë³´ ì¤‘ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° í•„ìš”í•œ í•µì‹¬ ë¬¸ì„œê°€ ìƒìœ„ ìˆœìœ„ì— ìž˜ ë°°ì¹˜ë˜ì—ˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "context_recall": {
                    "display_name": "ë¬¸ë§¥ ìž¬í˜„ìœ¨ (Context Recall)",
                    "reasoning": "ì •ë‹µì„ ìž‘ì„±í•˜ëŠ” ë° í•„ìš”í•œ ì‹¤ì œ ì •ë³´ë“¤ì´ ê²€ìƒ‰ëœ ë¬¸ë§¥ ë‚´ì— ëª¨ë‘ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "context_entity_recall": {
                    "display_name": "ê°œì²´ ìž¬í˜„ìœ¨ (Context Entity Recall)",
                    "reasoning": "ê¸°ì¤€ ì •ë‹µì— í¬í•¨ëœ í•µì‹¬ ê°œì²´(Entity)ë“¤ì´ ê²€ìƒ‰ëœ ë¬¸ë§¥ ë‚´ì— ì–¼ë§ˆë‚˜ ìž˜ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "answer_similarity": {
                    "display_name": "ë‹µë³€ ìœ ì‚¬ë„ (Answer Similarity)",
                    "reasoning": "ìƒì„±ëœ ë‹µë³€ê³¼ ê¸°ì¤€ ì •ë‹µ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë²¡í„° ê³µê°„ì—ì„œ ì¸¡ì •í•©ë‹ˆë‹¤."
                },
                "conciseness": {
                    "display_name": "ê°„ê²°ì„± (Conciseness)",
                    "reasoning": "ë‹µë³€ì´ ë¶ˆí•„ìš”í•œ ì‚¬ì¡± ì—†ì´ í•µì‹¬ì ì¸ ì •ë³´ë§Œ ê°„ê²°í•˜ê²Œ ì „ë‹¬í•˜ëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "coherence": {
                    "display_name": "ì¼ê´€ì„± (Coherence)",
                    "reasoning": "ë‹µë³€ì˜ ë¬¸ìž¥ íë¦„ê³¼ êµ¬ì¡°ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìžˆëŠ”ì§€ í‰ê°€í•©ë‹ˆë‹¤."
                },
                "harmfulness": {
                    "display_name": "ìœ í•´ì„± (Harmfulness)",
                    "reasoning": "ë‹µë³€ì— ì‚¬ìš©ìžì—ê²Œ ë¶ˆì¾Œê°ì„ ì£¼ê±°ë‚˜ ìœ í•´í•œ ë‚´ìš©ì´ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
                },
                "maliciousness": {
                    "display_name": "ì•…ì˜ì„± (Maliciousness)",
                    "reasoning": "ë‹µë³€ì— ê¸°ë§Œì ì´ê±°ë‚˜ ì•…ì˜ì ì¸ ì˜ë„ê°€ í¬í•¨ë˜ì–´ ìžˆëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤."
                }
            }
            
            for key, info in metric_info.items():
                if key in scores:
                    value = scores[key]
                    # Handle numpy types or lists that might come back
                    if isinstance(value, (list, tuple)) and len(value) > 0:
                        value = value[0]
                    
                    output.append(EvaluationResult(
                        score=float(value),
                        reasoning=info["reasoning"],
                        metric_name=info["display_name"]
                    ))
            
            return output
        except Exception as e:
            print(f"âš ï¸ Ragas evaluation failed: {e}")
            traceback.print_exc()
            return []

    # Keeping legacy methods for backward compatibility but re-routing to Ragas
    async def evaluate_faithfulness(self, context: str, answer: str) -> EvaluationResult:
        results = await self.run_ragas_eval("General Query", context, answer)
        for r in results:
            if "Faithfulness" in r.metric_name:
                return r
        return EvaluationResult(0.0, "Ragas Failed", "Faithfulness")

    async def evaluate_relevance(self, query: str, answer: str) -> EvaluationResult:
        # We need context for answer_relevancy in Ragas usually, or it uses the question embedding
        results = await self.run_ragas_eval(query, "No Context Provided", answer)
        for r in results:
            if "Relevancy" in r.metric_name:
                return r
        return EvaluationResult(0.0, "Ragas Failed", "Answer Relevancy")

evaluator = Evaluator()
