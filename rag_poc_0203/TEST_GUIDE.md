# 🧪 RAG 시스템 핸즈온 테스트 가이드 (Hands-on Guide)

이 문서는 구축된 MVP 시스템이 실제로 어떻게 동작하는지 단계별로 테스트하고, Langfuse와 Ragas를 통해 품질을 검증하는 방법을 안내합니다.

---

## 0. 시작 전 준비사항
1. **터미널 1**: Backend 실행 (`poetry run uvicorn app.api.server:app --reload`)
2. **터미널 2**: Frontend 실행 (`poetry run streamlit run app/ui/streamlit_app.py`)
3. **브라우저 접속**: `http://localhost:8501`

---

## 1. 단계별 테스트 시나리오

### **Step 1: 지식 데이터 적재 (Ingestion)**
시스템이 아무것도 모르는 상태에서 새로운 지식을 가르쳐봅니다.
1.  상단 탭에서 **[📊 데이터셋 관리]**를 클릭합니다.
2.  **데이터 적재 패널**을 열고, 테스트용 파일(`PDF` 추천)을 업로드합니다.
3.  **[🚀 데이터 적재 실행]** 버튼을 누릅니다. 
4.  하단에 "적재 완료" 메시지와 함께 파싱된 마크다운 내용이 보이면 성공입니다.

### **Step 2: 에이전틱 채팅 테스트 (Chat & Reasoning)**
단순 답변이 아닌, AI의 사고 과정을 테스트합니다.
1.  상단 탭에서 **[💬 채팅 & 테스트]**를 클릭합니다.
2.  왼쪽 사이드바에서 **🧩 Advanced Agent** 모드를 선택합니다.
3.  방금 적재한 문서와 관련된 복잡한 질문을 던집니다.
    - 예: "이 문서의 핵심 내용을 3가지로 요약하고, 그 근거를 본문에서 찾아줘."
4.  AI가 "생각 중..." 단계를 거쳐 답변을 내놓는지 확인합니다.

### **Step 3: 관측성 확인 (Observability)**
AI가 내부적으로 어떻게 움직였는지 랭퓨즈에서 확인합니다.
1.  질문 완료 후 사이드바에 나타난 **[Langfuse에서 보기]** 링크를 클릭합니다.
2.  **Trace** 화면에서 다음을 확인하세요:
    - `planner`: 질문을 어떻게 해석했는가?
    - `retriever`: Qdrant에서 어떤 문서를 찾아왔는가?
    - `executor`: 최종 프롬프트는 어떤 모양이었는가?
    - `critic`: 비평 점수가 몇 점이었고, 재시도(Retry)가 발생했는가?

### **Step 4: 성능 지표 평가 (Ragas Evaluation)**
이제 시스템의 답변 품질을 숫자로 측정합니다.
1.  상단 탭에서 **[✅ 평가 실행]**을 클릭합니다.
2.  **[평가 시작]** 버튼을 누릅니다. (`scripts/run_eval.py`가 백그라운드에서 실행됩니다.)
3.  약 1~2분 후 Langfuse 대시보드의 **Scores** 탭을 확인합니다.
4.  `faithfulness`, `answer_relevancy` 점수가 기록되었는지 확인합니다. (0.0 ~ 1.0)

---

## 2. 장애 발생 시 체크리스트
- **Qdrant 연결 오류**: 컨테이너가 떠 있는지(`docker ps`), 혹은 `.env`의 `QDRANT_URL`이 맞는지 확인하세요.
- **LLM 호출 오류**: `OPENAI_API_KEY`가 유효한지 확인하세요.
- **점수 미출력**: Ragas 평가는 수 초에서 수 분이 소요될 수 있습니다. 터미널 로그를 확인하세요.

---
**즐거운 테스트 되시길 바랍니다!** 🚀
